<?xml version="1.0" encoding="UTF-8" standalone="no"?><Paper acl-id="P03-1070">  <Title>Towards a Model of Face-to-Face Grounding</Title>  <Section>    <SectionTitle>Abstract</SectionTitle>    <S>We investigate the <term class="other">verbal and nonverbal means</term> for <term class="other">grounding</term>, and propose a design for <term class="tech">embodied conversational agents</term> that relies on both kinds of <term class="other">signals</term> to establish <term class="other">common ground</term> in <term class="tech">human-computer interaction</term>.</S>    <S>We analyzed <term class="other">eye gaze</term>, <term class="other">head nods</term> and <term class="other">attentional focus</term> in the context of a direction-giving task.</S>    <S>The distribution of <term class="other">nonverbal behaviors</term> differed depending on the type of <term class="other">dialogue move</term> being grounded, and the overall pattern reflected a monitoring of lack of <term class="other">negative feedback</term>.</S>    <S>Based on these results, we present an <term class="tech">ECA</term> that uses <term class="other">verbal and nonverbal grounding acts</term> to update <term class="other">dialogue state</term>.</S>  </Section></Paper>