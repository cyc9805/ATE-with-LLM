<?xml version="1.0" encoding="UTF-8" standalone="no"?><Paper acl-id="I05-5003">  <Title>Using Machine Translation Evaluation Techniques to Determine Sentence-level Semantic Equivalence</Title>  <Section>    <SectionTitle>Abstract</SectionTitle>    <S>The task of <term class="measure(ment)">machine translation (MT) evaluation</term> is closely related to the task of <term class="tech">sentence-level semantic equivalence classification</term>.</S>    <S>This paper investigates the utility of applying standard <term class="measure(ment)">MT evaluation methods (BLEU, NIST, WER and PER)</term> to building <term class="tech">classifiers</term> to predict <term class="other">semantic equivalence</term> and <term class="other">entailment</term>.</S>      <!-- how about class linguistic metadata?! e.g., syntax, pos, etc. -->    <S>We also introduce a novel <term class="tech">classification method</term> based on <term class="measure(ment)">PER</term> which leverages <term class="other">part of speech information</term> of the <term class="other">words</term> contributing to the <term class="other">word matches and non-matches</term> in the <term class="other">sentence</term>.</S>    <S>Our results show that <term class="measure(ment)">MT evaluation techniques</term> are able to produce useful <term class="other">features</term> for <term class="tech">paraphrase classification</term> and to a lesser extent <term class="other">entailment</term>.</S>    <S>Our <term class="tech">technique</term> gives a substantial improvement in <term class="measure(ment)">paraphrase classification accuracy</term> over all of the other <term class="tech">models</term> used in the experiments.</S>  </Section></Paper>