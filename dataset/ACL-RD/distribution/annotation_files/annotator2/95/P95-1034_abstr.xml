<?xml version="1.0" encoding="UTF-8" standalone="no"?><Paper acl-id="P95-1034">  <Title>Two-Level, Many-Paths Generation</Title>  <Section>    <SectionTitle>Abstract</SectionTitle>    <S>         <term class="tech">Large-scale natural language generation</term> requires the integration of vast amounts of <term class="other">knowledge</term>: lexical, grammatical, and conceptual.</S>    <S>A <term class="tech">robust generator</term> must be able to operate well even when pieces of <term class="other">knowledge</term> are missing.</S>    <S>It must also be robust against <term class="other">incomplete or inaccurate inputs</term>.</S>    <S>To attack these problems, we have built a <term class="tech">hybrid generator</term>, in which gaps in <term class="other">symbolic knowledge</term> are filled by <term class="tech">statistical methods</term>.</S>    <S>We describe algorithms and show experimental results.</S>    <S>We also discuss how the <term class="model">hybrid generation model</term> can be used to simplify current <term class="tech">generators</term> and enhance their <term class="other">portability</term>, even when perfect <term class="other">knowledge</term> is in principle obtainable.</S>  </Section></Paper>