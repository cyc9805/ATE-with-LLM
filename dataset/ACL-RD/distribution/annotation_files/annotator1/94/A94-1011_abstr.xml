<?xml version="1.0" encoding="UTF-8" standalone="no"?><Paper acl-id="A94-1011">  <Title>Exploiting Sophisticated Representations for Document Retrieval</Title>  <Section>    <SectionTitle>Abstract</SectionTitle>    <S>The use of <term class="tech">NLP techniques</term> for <term class="tech">document classification</term> has not produced significant improvements in performance within the standard <term class="tech">term weighting statistical assignment paradigm</term> (Fagan 1987; Lewis, 1992bc; Buckley, 1993).</S>    <S>This perplexing fact needs both an explanation and a solution if the power of recently developed <term class="tech">NLP techniques</term> are to be successfully applied in <term class="tech">IR</term>.</S>    <S>A novel method for adding <term class="other">linguistic annotation</term> to <term class="lr">corpora</term> is presented which involves using a <term class="tech">statistical POS tagger</term> in conjunction with <term class="tech">unsupervised structure finding methods</term> to derive notions of <term class="other">noun group</term>, <term class="other">verb group</term>, and so on which is inherently extensible to more sophisticated <term class="other">annotation</term>, and does not require a <term class="lr">pre-tagged corpus</term> to fit.</S>    <S>One of the distinguishing features of a more <term class="other">linguistically sophisticated representation of documents</term> over a <term class="other">word set based representation</term> of them is that <term class="other">linguistically sophisticated units</term> are more frequently individually good predictors of <term class="other">document descriptors (keywords)</term> than single <term class="other">words</term> are.</S>    <S>This leads us to consider the assignment of <term class="other">descriptors</term> from individual <term class="other">phrases</term> rather than from the <term class="other">weighted sum</term> of a <term class="other">word set representation</term>.</S>    <S>We investigate how sets of individually high-precision <term class="other">rules</term> can result in a <term class="measure(ment)">low precision</term> when used together, and develop some theory about these probably-correct <term class="other">rules</term>.</S>    <S>We then proceed to repeat results which show that standard <term class="tech">statistical models</term> are not particularly suitable for exploiting <term class="other">linguistically sophisticated representations</term>, and show that a <term class="tech">statistically fitted rule-based model</term> provides significantly improved performance for sophisticated <term class="other">representations</term>.</S>    <S>It therefore shows that <term class="tech">statistical systems</term> can exploit <term class="other">sophisticated representations of documents</term>, and lends some support to the use of more <term class="other">linguistically sophisticated representations</term> for <term class="tech">document classification</term>.</S>    <S>This paper reports on work done for the <term class="other">LRE project SmTA double check</term>, which is creating a <term class="tech">PC based tool</term> to be used in the <term class="other">technical abstracting industry</term>.</S>  </Section></Paper>