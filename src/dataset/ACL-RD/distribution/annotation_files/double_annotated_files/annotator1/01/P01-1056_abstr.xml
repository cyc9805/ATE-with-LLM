<?xml version="1.0" encoding="UTF-8" standalone="no"?><Paper acl-id="P01-1056">	<Title>Evaluating a Trainable Sentence Planner for a Spoken Dialogue System</Title>		<Section>			<SectionTitle>Abstract</SectionTitle>			<S><term class="tech">Techniques for automatically training</term> modules of a <term class="tech">natural language generator</term> have recently been proposed, but a fundamental concern is whether the <term class="measure(ment)">quality</term> of <term class="other">utterances</term> produced with <term class="other">trainable components</term> can compete with <term class="tech">hand-crafted template-based or rule-based approaches</term>.</S>			<S>In this paper We experimentally evaluate a <term class="tech">trainable sentence planner</term> for a <term class="tech">spoken dialogue system</term> by eliciting <term class="other">subjective human judgments</term>.</S>			<S>In order to perform an exhaustive comparison, we also evaluate a <term class="tech">hand-crafted template-based generation component</term>, two <term class="tech">rule-based sentence planners</term>, and two <term class="tech">baseline sentence planners</term>.</S>			<S>We show that the <term class="tech">trainable sentence planner</term> performs better than the <term class="tech">rule-based systems</term> and the <term class="tech">baselines</term>, and as well as the <term class="tech">hand-crafted system</term>.</S>		</Section></Paper>