<?xml version="1.0" encoding="UTF-8" standalone="no"?><Paper acl-id="J05-1003"> <Title>Discriminative Reranking for Natural Language Parsing</Title> <Section>  <SectionTitle>1. Introduction</SectionTitle>  <S>This article considers approaches which rerank the output of an existing <term class="tech">probabilistic parser</term>.</S>  <S>The base <term class="tech">parser</term> produces a set of <term class="other">candidate parses</term> for each input <term class="other">sentence</term>, with associated <term class="other">probabilities</term> that define an initial <term class="other">ranking</term> of these <term class="other">parses</term>.</S> <!-- could we say parse tree (here, parse) is a model?! -->  <S>A second <term class="tech">model</term> then attempts to improve upon this initial <term class="other">ranking</term>, using additional <term class="other">features</term> of the <term class="other">tree</term> as evidence.</S>  <S>The strength of our approach is that it allows a <term class="other">tree</term> to be represented as an arbitrary set of <term class="other">features</term>, without concerns about how these <term class="other">features</term> interact or overlap and without the need to define a <term class="other">derivation</term> or a <term class="tech">generative model</term> which takes these <term class="other">features</term> into account.</S>  <S>We introduce a new method for the <term class="tech">reranking task</term>, based on the <term class="tech">boosting approach</term> to <term class="other">ranking problems</term> described in Freund et al. (1998).</S> <!-- are citations going to be annotated?! I say yes! although it is very rare in abstracts -->  <S>We apply the <term class="tech">boosting method</term> to <term class="tech">parsing</term> the <term class="lr-prod">Wall Street Journal treebank</term>.</S> <!-- log-likelihood is certainly a measure but certianly is not going to go in the category 6, we have to open a new category for these measures. -->  <S>The method combined the <term class="other">log-likelihood</term> under a <term class="model">baseline model</term> (that of Collins [1999]) with evidence from an additional 500,000 <term class="other">features</term> over <term class="other">parse trees</term> that were not included in the original <term class="model">model</term>.</S>  <S>The new <term class="tech">model</term> achieved 89.75% <term class="measure(ment)">F-measure</term>, a 13% relative decrease in <term class="measure(ment)">F-measure</term> error over the <term class="measure(ment)">baseline model’s score</term> of 88.2%.</S>  <S>The article also introduces a new algorithm for the <term class="tech">boosting approach</term> which takes advantage of the <term class="other">sparsity of the feature space</term> in the <term class="other">parsing data</term>.</S>  <S>Experiments show significant efficiency gains for the new algorithm over the obvious <term class="other">implementation</term> of the <term class="tech">boosting approach</term>.</S>  <S>We argue that the method is an appealing alternative—in terms of both simplicity and efficiency—to work on <term class="tech">feature selection methods</term> within <term class="tech">log-linear (maximum-entropy) models</term>.</S>  <S>Although the experiments in this article are on <term class="tech">natural language parsing (NLP)</term>, the approach should be applicable to many other <term class="other">NLP problems</term> which are naturally framed as <term class="tech">ranking tasks</term>, for example, <term class="tech">speech recognition</term>, <term class="tech">machine translation</term>, or <term class="tech">natural language generation</term>.</S> </Section></Paper>