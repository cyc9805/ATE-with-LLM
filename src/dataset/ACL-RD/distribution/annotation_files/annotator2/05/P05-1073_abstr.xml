<?xml version="1.0" encoding="UTF-8" standalone="no"?><Paper acl-id="P05-1073">  <Title>Joint Learning Improves Semantic Role Labeling</Title>  <Section>    <SectionTitle>Abstract</SectionTitle>    <S>Despite much recent progress on accurate <term class="tech">semantic role labeling</term>, previous work has largely used independent <term class="tech">classifiers</term>, possibly combined with separate <term class="model">label sequence models</term> via <term class="tech">Viterbi decoding</term>.</S>    <S>This stands in stark contrast to the linguistic observation that a <term class="other">core argument frame</term> is a joint structure, with strong <term class="other">dependencies</term> between <term class="other">arguments</term>.</S>    <S>We show how to build a joint <term class="model">model</term> of <term class="other">argument frames</term>, incorporating novel <term class="other">features</term> that model these interactions into <term class="model">discriminative log-linear models</term>.</S>    <S>This system achieves an <term class="measure(ment)">error reduction</term> of 22% on all <term class="other">arguments</term> and 32% on <term class="other">core arguments</term> over a state-of-the art independent <term class="tech">classifier</term> for <term class="other">gold-standard parse trees</term> on <term class="lr-prod">PropBank</term>.</S>  </Section></Paper>