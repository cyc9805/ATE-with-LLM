{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import f1_score, micro_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(input_text, text, delimiter=', ', is_pred=True):\n",
    "    if is_pred:\n",
    "        if \"let me know\" in text.lower() and (text.lower().endswith(\"! \\n\") or text.lower().endswith(\"!\\n\")):\n",
    "            text = text[:text.lower().index(\"let me know\")]\n",
    "\n",
    "        if \"## reason:\" in text.lower():\n",
    "            text = text[:text.lower().find(\"## reason:\")].strip()\n",
    "        \n",
    "        if \"**explanation:**\" in text.lower():\n",
    "            text = text[:text.lower().find(\"**explanation:**\")].strip()\n",
    "            \n",
    "        while text.startswith('\\n'):\n",
    "            text = text[1:]\n",
    "            \n",
    "        while text.endswith('\\n'):\n",
    "            text = text[:-1]\n",
    "            \n",
    "        # if '\\n' in text:\n",
    "        #     text = text.split('\\n')[-1].strip()\n",
    "            \n",
    "        lowered_text = text.lower()\n",
    "        lowered_input_text = input_text.lower()\n",
    "        \n",
    "        keywords1 = ['answer:', 'categorize terms:', 'output:', 'are:', 'is:', 'terms:', 'domain:', 'terms**', 'output**', '**output:**']\n",
    "        keywords2 = ['terms are', 'term is', 'should be:', 'would be:', 'answer is:', 'answer is ', 'output is:', 'the annotator should write']\n",
    "        for keyword in keywords1+keywords2:\n",
    "            if keyword in lowered_text:\n",
    "                if keyword in keywords2 and lowered_text not in lowered_input_text:\n",
    "                    text = text[lowered_text.find(keyword)+len(keyword):].strip()\n",
    "                else:\n",
    "                    text = text[lowered_text.rfind(keyword)+len(keyword):].strip()\n",
    "                break\n",
    "        \n",
    "        # Eliminate any special characters at the prefix\n",
    "        pattern1 = r'^[^\\w]+'\n",
    "        text = re.sub(pattern1, ' ', text)\n",
    "    \n",
    "    # Eliminate parantheses\n",
    "    pattern2 = r'[\\[\\]\\\"\\']'\n",
    "    text = re.sub(pattern2, ' ', text)\n",
    "        \n",
    "    domain_words = []\n",
    "    for domain_word in text.split(delimiter):\n",
    "        while domain_word.strip().startswith('.') or domain_word.strip().endswith('.'):\n",
    "            domain_word = domain_word.replace('.', ' ')\n",
    "        domain_words.append(domain_word)\n",
    "    \n",
    "    if is_pred:\n",
    "        splited_domain_words = []\n",
    "        for domain_word in domain_words:\n",
    "            lowered_domain_word = domain_word.lower()\n",
    "            if lowered_domain_word.startswith('and ') or ' and ' in lowered_domain_word and lowered_domain_word.strip() not in lowered_input_text:\n",
    "                domain_words.remove(domain_word)\n",
    "                for splited_domain_word in domain_word.split('and '):\n",
    "                    splited_domain_word = splited_domain_word.strip()\n",
    "                    if splited_domain_word != '':\n",
    "                        splited_domain_words.append(splited_domain_word)\n",
    "        \n",
    "        domain_words.extend(splited_domain_words)\n",
    "        # check_for_colon = all(map(lambda x: ':' in x, domain_words))\n",
    "        # if check_for_colon:\n",
    "        #     domain_words = list(map(lambda x: x.split(':')[0].strip(), domain_words))\n",
    "        # Check for repetitive answers\n",
    "        cnt_threshold = 50\n",
    "        cnt = 0\n",
    "        for i in range(len(domain_words)):\n",
    "            for j in range(i+1, len(domain_words)):\n",
    "                if domain_words[i] == domain_words[j]:\n",
    "                    cnt += 1\n",
    "                if cnt > cnt_threshold:\n",
    "                    domain_words = list(set(domain_words))\n",
    "                    print('Repetitive answers are removed:', domain_words)\n",
    "                    break\n",
    "            if cnt > cnt_threshold:\n",
    "                break\n",
    "            \n",
    "    domain_words = list(map(lambda x: x.strip(), domain_words))\n",
    "    return domain_words   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from glob import glob\n",
    "\n",
    "master_path = 'outputs/test'\n",
    "reports = None\n",
    "\n",
    "empty_paths = []\n",
    "exclude_recalcuation_models = ['bart', 'roberta', 'mbart']\n",
    "# Manually selected paths\n",
    "unwanted_paths = []\n",
    "really_calculate_f1_score = True\n",
    "really_store_unwnated_paths = False\n",
    "\n",
    "select_columns = ['model_name', 'dataset_name', 'num_shots', 'retrieval_method', \n",
    "                  'total_precision', 'total_recall', \n",
    "                  'micro_f1_score', \n",
    "                  'seed', 'individual_report_path',\n",
    "                  'f1_score', 'precision', 'preprocessed_refs']\n",
    "\n",
    "for dataset_name in os.listdir(master_path):\n",
    "    dataset_path = os.path.join(master_path, dataset_name)\n",
    "    for retrieval_style in os.listdir(dataset_path):\n",
    "        retrieval_style_path = os.path.join(dataset_path, retrieval_style, \"[0-9][0-9]_[0-9][0-9]_[0-9][0-9]_[0-9][0-9]_[0-9][0-9]\")\n",
    "        for output in glob(retrieval_style_path):\n",
    "            report_path = os.path.join(output, 'report')\n",
    "            individual_report_path = os.path.join(report_path, 'report_0.csv')\n",
    "            overall_report_path = os.path.join(report_path, 'report_overall.csv')\n",
    "            config_path = os.path.join(output, 'config.json')\n",
    "            \n",
    "            try:\n",
    "                with open(overall_report_path, 'r') as f:\n",
    "                    overall_report = json.load(f)\n",
    "                \n",
    "                with open(config_path, 'r') as f:\n",
    "                    config = json.load(f)\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}, overall_report_path: {overall_report_path}, config_path: {config_path}\")\n",
    "                empty_paths.append(output)\n",
    "                continue  \n",
    "            ############## Manually selected paths ##############\n",
    "            if really_store_unwnated_paths and 'retrieval_method' in config and config['prompt_style'] == 'default' and config['retrieval_method'] == 'default' and 'mbart' not in config['model_name'] and 'roberta' not in config['model_name']:\n",
    "                unwanted_paths.append(output)\n",
    "            ####################################################\n",
    "                \n",
    "            config['model_name'] = os.path.basename(config['model_name'])\n",
    "            overall_report = pd.DataFrame([overall_report])\n",
    "            config = pd.DataFrame([config])\n",
    "            individual_report_path_df = pd.DataFrame({\"individual_report_path\": individual_report_path}, index=[0])\n",
    "            report = pd.concat([overall_report, config, individual_report_path_df], axis=1)\n",
    "            \n",
    "            if reports is None:\n",
    "                reports = report\n",
    "                continue\n",
    "            else:\n",
    "                reports = pd.concat([reports, report], axis=0)\n",
    "                \n",
    "duplicate_columns = select_columns.copy().remove('individual_report_path')\n",
    "reports = reports[select_columns]\n",
    "reports = reports.drop_duplicates(subset=duplicate_columns).reset_index(drop=True)\n",
    "reports.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from scipy.stats import spearmanr\n",
    "import numpy as np\n",
    "\n",
    "retrieval_method_to_model_name = {  \n",
    "                                    'fastkassim': '',\n",
    "                                    'default':'bge-large-en-v1.5', \n",
    "                                    'default_w_ins': 'bge-en-icl', \n",
    "                                    'bm25': '',\n",
    "                                    'random': '',\n",
    "                                    }\n",
    "retrieval_method_dfs = []\n",
    "model_names = ['gemma-2-9b-it', 'Meta-Llama-3.1-8B-Instruct', 'Mistral-Nemo-Instruct-2407']\n",
    "dataset_names = ['ACTER', 'ACL-RD', 'BCGM']\n",
    "\n",
    "# Select rows where model_name is in model_names\n",
    "retrieval_method_df = reports[reports['model_name'].isin(model_names)]\n",
    "retrieval_method_df = retrieval_method_df[retrieval_method_df['dataset_name'].isin(dataset_names)]\n",
    "retrieval_method_df = retrieval_method_df[retrieval_method_df['num_shots']!=0.0]\n",
    "retrieval_method_df = retrieval_method_df[retrieval_method_df['num_shots'].isin([5])]   \n",
    "retrieval_method_df = retrieval_method_df[retrieval_method_df['seed'].isin([42, 1000, 2000, 3000])]   \n",
    "\n",
    "in_domain_datasets = ['ACL-RD','BCGM']\n",
    "cross_domain_datasets = ['ACTER']\n",
    "\n",
    "for retrieval_method in retrieval_method_to_model_name:\n",
    "    cross_domain = {'correlation': [], 'overlap_ratio': []}\n",
    "    in_domain = {'correlation': [], 'overlap_ratio': []}\n",
    "    for dataset_name in dataset_names:\n",
    "        corr_list = []\n",
    "        label_frequency_list = []\n",
    "        ds_df = retrieval_method_df[retrieval_method_df['dataset_name'] == dataset_name]\n",
    "        for model_name in model_names:\n",
    "            df1 = ds_df[ds_df['retrieval_method'] == retrieval_method]\n",
    "            df2 = df1[df1['model_name']==model_name]   \n",
    "            label_frequency = []\n",
    "            f1_scores = []\n",
    "        \n",
    "            for i in range(df2.shape[0]):\n",
    "                row = df2.iloc[i]\n",
    "                preprocessed_refs = row['preprocessed_refs']\n",
    "                retrieved_result = row['retrieved_result']\n",
    "                if not isinstance(retrieved_result, list) or not isinstance(preprocessed_refs, str):\n",
    "                    continue\n",
    "                \n",
    "                preprocessed_refs = literal_eval(preprocessed_refs)\n",
    "                f1_score = [float(f1_score) for f1_score in row['f1_score'].split(',')]\n",
    "                total_tp = 0\n",
    "                total_pr = 0\n",
    "                for rr, pr, fs in zip(retrieved_result, preprocessed_refs, f1_score):\n",
    "                    if pr == ['No term']:\n",
    "                        continue\n",
    "                    \n",
    "                    label_set = set()\n",
    "                    for r in rr:\n",
    "                        label = r['label']\n",
    "                        if len(label) == 0:\n",
    "                            label = ['No term']\n",
    "\n",
    "                        for l in label:\n",
    "                            label_set.add(l.lower())\n",
    "\n",
    "                    pr = set([p.lower() for p in pr])\n",
    "                    tp = label_set & pr\n",
    "                    total_tp += len(tp)\n",
    "                    total_pr += len(pr)\n",
    "\n",
    "                    precision = len(tp)/len(pr)\n",
    "                    recall = len(tp)/len(pr)\n",
    "\n",
    "                    label_frequency.append(precision)\n",
    "                    f1_scores.append(fs)\n",
    "\n",
    "            if np.all(np.array(label_frequency)==0.0) or np.all(np.array(f1_scores)==0.0):\n",
    "                corr, p_value = 0, 0\n",
    "            elif len(label_frequency) == 0 or len(f1_scores) == 0:\n",
    "                corr, p_value = np.nan, np.nan\n",
    "                print('Correlation is nan')\n",
    "            else:\n",
    "\n",
    "                corr, p_value = spearmanr(f1_scores, label_frequency)\n",
    "                \n",
    "                corr_list.append(corr)\n",
    "                label_frequency_list.extend(label_frequency)\n",
    "        \n",
    "        if dataset_name in in_domain_datasets:\n",
    "            in_domain['correlation'].extend(corr_list)\n",
    "            in_domain['overlap_ratio'].extend(label_frequency_list)\n",
    "        elif dataset_name in cross_domain_datasets:\n",
    "            cross_domain['correlation'].extend(corr_list)\n",
    "            cross_domain['overlap_ratio'].extend(label_frequency_list)\n",
    "            \n",
    "    print(f\"Cross-domain method: {retrieval_method}, correlation: {np.mean(cross_domain['correlation'])*100}, overlap_ratio:{np.mean(cross_domain['overlap_ratio'])*100}\")\n",
    "    print(f\"In-domain method: {retrieval_method}, correlation: {np.mean(in_domain['correlation'])*100}, overlap_ratio:{np.mean(in_domain['overlap_ratio'])*100}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison between different retrieval methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plm_model_names = ['roberta-large', 'bart-large']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "figure_output_dir = 'figures'\n",
    "colors =  {'zero_shot': 'yellow', 'default':'blue', 'default_w_ins':'purple', 'random':'green', 'fastkassim':'red', 'bm25':'orange'}\n",
    "dataset_name_to_title = {'ACTER':'ACTER', 'ACL-RD':'ACL RD-TEC 2.0', 'BCGM':'BCGM'}\n",
    "retrieval_method_to_legend = {'zero_shot': 'Zero-shot', 'default':'BGE-large-en', 'default_w_ins':'BGE-en-icl', 'random':'Random', 'fastkassim':'FastKASSIM', 'bm25': 'BM25'}\n",
    "default_criteria = [list(reports['temperature']==0.01), \n",
    "                    list(reports['do_sample']==False), \n",
    "                    list(reports['seed'].isin([42, 1000, 2000, 3000]))]\n",
    "best_syntatic_score = {}\n",
    "best_semantic_score = {}\n",
    "\n",
    "correlations_per_method = {'default':[], 'random':[], 'fastkassim':[], 'bm25':[], 'default_w_ins':[]}\n",
    "for model_name in ['gemma-2-9b-it', 'Meta-Llama-3.1-8B-Instruct', 'Mistral-Nemo-Instruct-2407']:\n",
    "    for dataset_name in dataset_name_to_title.keys():\n",
    "        for prompt_style in ['default']:    \n",
    "            identical_criteria = default_criteria + [list(reports['dataset_name']==dataset_name), list(reports['prompt_style'] == prompt_style), list(reports['model_name']==model_name)]\n",
    "            \n",
    "            identical_criteria = identical_criteria + [list(reports['num_shots']!=0)]\n",
    "            meets_criteria = []\n",
    "            criteria = identical_criteria + [list(reports['retrieval_method']=='default'), list(reports['seed']==42), list(reports['default_prompt_style']==0)]  # Defulat prompt style이 0이 아닌 경우도 있어야 함\n",
    "            for i in zip(*criteria):\n",
    "                meets_criteria.append(all(i))\n",
    "            default_reports = reports[meets_criteria]\n",
    "            \n",
    "            meets_criteria = []\n",
    "            criteria = identical_criteria + [list(reports['retrieval_method']=='fastkassim'), list(reports['default_prompt_style']==0)]\n",
    "            for i in zip(*criteria):\n",
    "                meets_criteria.append(all(i))\n",
    "            fastkassim_reports = reports[meets_criteria]\n",
    "\n",
    "            meets_criteria = []\n",
    "            criteria = identical_criteria + [list(reports['retrieval_method']=='bm25')]\n",
    "            for i in zip(*criteria):\n",
    "                meets_criteria.append(all(i))\n",
    "            bm25_reports = reports[meets_criteria]\n",
    "\n",
    "            for r in bm25_reports.iterrows():\n",
    "                print(f\"Dataset: {dataset_name}, Model: {model_name}, Retrieval method: bm25\")\n",
    "                print(f\"Num Shots: {r[1]['num_shots']}, Precision: {r[1]['total_precision']} Recall: {r[1]['total_precision']} F1 score: {r[1]['micro_f1_score']}\")\n",
    "                \n",
    "            meets_criteria = []\n",
    "            criteria = identical_criteria + [list(reports['retrieval_method']=='random'), list(reports['seed'].isin([1000, 3000]))]\n",
    "            for i in zip(*criteria):\n",
    "                meets_criteria.append(all(i))\n",
    "\n",
    "            random_reports = reports[meets_criteria]  \n",
    "            \n",
    "            meets_criteria = []\n",
    "            criteria = identical_criteria + [list(reports['retrieval_method']=='default_w_ins')]\n",
    "            for i in zip(*criteria):\n",
    "                meets_criteria.append(all(i))\n",
    "\n",
    "            dwi_reports = reports[meets_criteria]  \n",
    "\n",
    "            meets_criteria = []\n",
    "            criteria = identical_criteria + [list(reports['retrieval_method']=='bm25')]\n",
    "            for i in zip(*criteria):\n",
    "                meets_criteria.append(all(i))\n",
    "\n",
    "            bm25_reports = reports[meets_criteria]  \n",
    "\n",
    "            default_reports = default_reports.sort_values('num_shots')\n",
    "            random_reports = random_reports.sort_values('num_shots')\n",
    "            bm25_reports = bm25_reports.sort_values('num_shots')\n",
    "            fastkassim_reports = fastkassim_reports.sort_values('num_shots')\n",
    "            dwi_reports = dwi_reports.sort_values('num_shots')\n",
    "\n",
    "            plt.title(f\"{model_name} performance on {dataset_name_to_title[dataset_name]}\", fontweight='bold')\n",
    "            plt.xlabel('Number of Demonstrations')\n",
    "            plt.ylabel('Micro F1 Score')\n",
    "            plt.xticks(default_reports['num_shots'])\n",
    "            for selected_reports in [default_reports, dwi_reports, random_reports, fastkassim_reports, bm25_reports]:\n",
    "                try:\n",
    "                    _model_name = selected_reports['model_name'].iloc[0]\n",
    "                    _retrieval_method = selected_reports['retrieval_method'].iloc[0]\n",
    "                    color = colors[selected_reports['retrieval_method'].values[0]]\n",
    "                    if selected_reports is random_reports:\n",
    "                        random_mean_scores = selected_reports.groupby('num_shots')['micro_f1_score'].mean()\n",
    "                        ramdom_prec_mean_scores = selected_reports.groupby('num_shots')['total_precision'].mean()\n",
    "                        random_recall_mean_scores = selected_reports.groupby('num_shots')['total_recall'].mean()\n",
    "                        corr = np.corrcoef(selected_reports['num_shots'], selected_reports['micro_f1_score'])[0][1]\n",
    "                        correlations_per_method[selected_reports['retrieval_method'].values[0]].append(corr)\n",
    "                        \n",
    "                        sns.lineplot(x='num_shots', y='micro_f1_score', data=selected_reports, ci=95, label='Random', color=color)\n",
    "                        plt.scatter(default_reports['num_shots'], random_mean_scores, color=color)\n",
    "                    else:\n",
    "                        figure_label = retrieval_method_to_legend[selected_reports['retrieval_method'].values[0]]\n",
    "                        plt.plot(selected_reports['num_shots'], selected_reports['micro_f1_score'], label=figure_label, color=color)\n",
    "                        plt.scatter(selected_reports['num_shots'], selected_reports['micro_f1_score'], color=color)\n",
    "                        corr = np.corrcoef(selected_reports['num_shots'], selected_reports['micro_f1_score'])[0][1]\n",
    "                        correlations_per_method[selected_reports['retrieval_method'].values[0]].append(corr)\n",
    "                        print(f\"{model_name} correlation on {dataset_name_to_title[dataset_name]} for {figure_label} is {corr}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {e}\")\n",
    "                    continue\n",
    "            plt.legend()\n",
    "            # Save the figure\n",
    "            plt.savefig(os.path.join(figure_output_dir, f\"{model_name}_{dataset_name}_{prompt_style}.png\"))\n",
    "            plt.show()\n",
    "            default_best_score = max(default_reports['micro_f1_score'])\n",
    "            default_best_precision = default_reports[default_reports['micro_f1_score'] == default_best_score]['total_precision'].values[0]\n",
    "            default_best_recall = default_reports[default_reports['micro_f1_score'] == default_best_score]['total_recall'].values[0]\n",
    "        \n",
    "            default_10_report = default_reports[default_reports['num_shots'] == 10]\n",
    "            default_10_em = default_10_report['total_em_score'].values[0]*100\n",
    "            default_10_score = default_10_report['micro_f1_score'].values[0]\n",
    "            default_10_precision = default_10_report['total_precision'].values[0]\n",
    "            default_10_recall = default_10_report['total_recall'].values[0]\n",
    "            \n",
    "            num_shot_default_best_score = default_reports[default_reports['micro_f1_score'] == default_best_score]['num_shots'].values[0]   \n",
    "            \n",
    "            random_best_score = max(random_mean_scores)\n",
    "            random_best_precision = max(ramdom_prec_mean_scores)\n",
    "            random_best_recall = max(random_recall_mean_scores)\n",
    "            \n",
    "            random_10_report = random_reports[random_reports['num_shots'] == 10]\n",
    "            random_10_em = random_10_report['total_em_score'].mean()*100\n",
    "            random_10_score = random_10_report['micro_f1_score'].mean()\n",
    "            random_10_precision = random_10_report['total_precision'].mean()\n",
    "            random_10_recall = random_10_report['total_recall'].mean()\n",
    "            \n",
    "            fastkassim_best_score = max(fastkassim_reports['micro_f1_score'])\n",
    "            fastkassim_best_precision = fastkassim_reports[fastkassim_reports['micro_f1_score'] == fastkassim_best_score]['total_precision'].values[0]\n",
    "            fastkassim_best_recall = fastkassim_reports[fastkassim_reports['micro_f1_score'] == fastkassim_best_score]['total_recall'].values[0]\n",
    "            \n",
    "            fastkassim_10_report = fastkassim_reports[fastkassim_reports['num_shots'] == 10]\n",
    "            fastkassim_10_em = fastkassim_10_report['total_em_score'].values[0]*100\n",
    "            fastkassim_10_score = fastkassim_10_report['micro_f1_score'].values[0]\n",
    "            fastkassim_10_precision = fastkassim_10_report['total_precision'].values[0]\n",
    "            fastkassim_10_recall = fastkassim_10_report['total_recall'].values[0]\n",
    "            \n",
    "            num_shot_fastkassim_best_score = fastkassim_reports[fastkassim_reports['micro_f1_score'] == fastkassim_best_score]['num_shots'].values[0]\n",
    "            \n",
    "            dwi_best_score = max(dwi_reports['micro_f1_score'])\n",
    "            dwi_best_precision = dwi_reports[dwi_reports['micro_f1_score'] == dwi_best_score]['total_precision'].values[0]\n",
    "            dwi_best_recall = dwi_reports[dwi_reports['micro_f1_score'] == dwi_best_score]['total_recall'].values[0]\n",
    "            \n",
    "            dwi_10_report = dwi_reports[dwi_reports['num_shots']==10]\n",
    "            dwi_10_em = dwi_10_report['total_em_score'].values[0]*100\n",
    "            dwi_10_score = dwi_10_report['micro_f1_score'].values[0]\n",
    "            dwi_10_precision = dwi_10_report['total_precision'].values[0]\n",
    "            dwi_10_recall = dwi_10_report['total_recall'].values[0]\n",
    "            \n",
    "            num_shot_dwi_best_score = dwi_reports[dwi_reports['micro_f1_score'] == dwi_best_score]['num_shots'].values[0]\n",
    "\n",
    "            \n",
    "            bm25_best_score = max(bm25_reports['micro_f1_score'])\n",
    "            bm25_best_precision = bm25_reports[bm25_reports['micro_f1_score'] == bm25_best_score]['total_precision'].values[0]\n",
    "            bm25_best_recall = bm25_reports[bm25_reports['micro_f1_score'] == bm25_best_score]['total_recall'].values[0]\n",
    "        \n",
    "            bm25_10_report = bm25_reports[bm25_reports['num_shots'] == 10]\n",
    "            bm25_10_em = bm25_10_report['total_em_score'].values[0]*100\n",
    "            bm25_10_score = bm25_10_report['micro_f1_score'].values[0]\n",
    "            bm25_10_precision = bm25_10_report['total_precision'].values[0]\n",
    "            bm25_10_recall = bm25_10_report['total_recall'].values[0]\n",
    "            \n",
    "            num_shot_bm25_best_score = bm25_reports[bm25_reports['micro_f1_score'] == bm25_best_score]['num_shots'].values[0]\n",
    "            \n",
    "            if dataset_name in ['ACTER', 'GENIA_to_ACL-RD', 'ACL-RD_to_GENIA']:\n",
    "                best_syntatic_score[model_name] = fastkassim_best_score\n",
    "            elif dataset_name in ['ACL-RD', 'GENIA']:\n",
    "                best_semantic_score[model_name] = default_best_score\n",
    "            \n",
    "            print(f\"#################################### REPORTS ####################################\")\n",
    "            print(f\"Model: {model_name}, Dataset: {dataset_name}, Prompt style: {prompt_style}\")\n",
    "            print(\"Default best score - precision: {:.1f}, recall: {:.1f}, f1 score: {:.1f}\".format(default_best_precision, default_best_recall, default_best_score), f\"at {num_shot_default_best_score} shots\")\n",
    "            print('Default with Instruction best score - precision: {:.1f}, recall: {:.1f}, f1 score: {:.1f}'.format(dwi_best_precision, dwi_best_recall, dwi_best_score), f\"at {num_shot_dwi_best_score} shots\")\n",
    "            print('BM25 best score - precision: {:.1f}, recall: {:.1f}, f1 score: {:.1f}'.format(bm25_best_precision, bm25_best_recall, bm25_best_score), f\"at {num_shot_bm25_best_score} shots\")\n",
    "            print('Random best score - precision: {:.1f}, recall: {:.1f}, f1 score: {:.1f}'.format(random_best_precision, random_best_recall, random_best_score), f\"at {num_shot_fastkassim_best_score} shots\")\n",
    "            print('Fastkassim best score - precision: {:.1f}, recall: {:.1f}, f1 score: {:.1f}'.format(fastkassim_best_precision, fastkassim_best_recall, fastkassim_best_score), f\"at {num_shot_fastkassim_best_score} shots\")\n",
    "            print()\n",
    "            # print('Relative similarity best score - precision: {:.1f}, recall: {:.1f}, f1 score: {:.1f}'.format(relative_similarity_best_precision*100, relative_similarity_best_recall*100, relative_similarity_best_score*100), f\"at {num_shot_relative_similarity_best_score} shots\")\n",
    "\n",
    "            print(\"Default 10 shots - precision: {:.1f}, recall: {:.1f}, f1 score: {:.1f}, EM: {:.1f}\".format(default_10_precision, default_10_recall, default_10_score, default_10_em))\n",
    "            print('Default with Instruction 10 shots - precision: {:.1f}, recall: {:.1f}, f1 score: {:.1f}, EM: {:.1f}'.format(dwi_10_precision, dwi_10_recall, dwi_10_score, dwi_10_em))\n",
    "            print('BM25 10 shots - precision: {:.1f}, recall: {:.1f}, f1 score: {:.1f}, EM: {:.1f}'.format(bm25_10_precision, bm25_10_recall, bm25_10_score, bm25_10_em))\n",
    "            print('Random 10 shots - precision: {:.1f}, recall: {:.1f}, f1 score: {:.1f}, EM: {:.1f}'.format(random_10_precision, random_10_recall, random_10_score, random_10_em))\n",
    "            print('Fastkassim 10 shots - precision: {:.1f}, recall: {:.1f}, f1 score: {:.1f}, EM: {:.1f}'.format(fastkassim_10_precision, fastkassim_10_recall, fastkassim_10_score, fastkassim_10_em))\n",
    "\n",
    "            random_scores = list(random_reports.groupby('num_shots')['micro_f1_score'])\n",
    "            \n",
    "            # Calculate the margin of error\n",
    "            for ramdom_prec_mean_score, random_recall_mean_score, random_mean_score, random_score in zip(ramdom_prec_mean_scores, random_recall_mean_scores, random_mean_scores, random_scores):\n",
    "                random_score = random_score[1].values\n",
    "                confidence_level = 0.95\n",
    "                sem = stats.sem(random_score)\n",
    "                degrees_of_freedom = len(random_score) - 1\n",
    "\n",
    "                if random_10_score == random_mean_score:\n",
    "                    f1_confidence_interval = stats.t.interval(confidence_level, degrees_of_freedom, random_mean_score, sem)\n",
    "                    prec_confidence_interval = stats.t.interval(confidence_level, degrees_of_freedom, ramdom_prec_mean_score, sem) \n",
    "                    recall_confidence_interval = stats.t.interval(confidence_level, degrees_of_freedom, random_recall_mean_score, sem)\n",
    "                    \n",
    "                    print('Confidence interval for F1 random score: {:.1f}, inverval: {:.1f} to {:.1f}, margin of error: {:.1f}'.format(random_mean_score, f1_confidence_interval[0], f1_confidence_interval[1], (f1_confidence_interval[1]-random_mean_score)))\n",
    "                    print('Confidence interval for Precision random score: {:.1f}, inverval: {:.1f} to {:.1f}, margin of error: {:.1f}'.format(ramdom_prec_mean_score, prec_confidence_interval[0], prec_confidence_interval[1], (prec_confidence_interval[1]-ramdom_prec_mean_score)))\n",
    "                    print('Confidence interval for Recall random score: {:.1f}, inverval: {:.1f} to {:.1f}, margin of error: {:.1f}'.format(random_recall_mean_score, recall_confidence_interval[0], recall_confidence_interval[1], (recall_confidence_interval[1]-random_recall_mean_score)))\n",
    "            print(f\"#####################################################################################\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline performance\n",
    "best_plm_score_acter = {}\n",
    "best_plm_score_acl_rd = {}\n",
    "for model_name in plm_model_names:\n",
    "    for dataset_name in ['ACTER', 'ACL-RD']:\n",
    "        criteria = [list(reports['dataset_name']==dataset_name), list(reports['model_name']==model_name)]\n",
    "        meets_criteria = []\n",
    "        for i in zip(*criteria):\n",
    "            meets_criteria.append(all(i))\n",
    "        plm_reports = reports[meets_criteria]\n",
    "        best_plm_score = plm_reports['micro_f1_score'].max()\n",
    "        best_prec_score = plm_reports[plm_reports['micro_f1_score'] == best_plm_score]['total_precision'].values[0]\n",
    "        best_recall_score = plm_reports[plm_reports['micro_f1_score'] == best_plm_score]['total_recall'].values[0]\n",
    "        \n",
    "        print('Datset name: {}, Model name:{}, micro f1 score: {:.1f}, precision: {:.1f}, recall: {:.1f}'.format(dataset_name, model_name, best_plm_score*100, best_prec_score*100, best_recall_score*100))\n",
    "        if dataset_name == 'ACTER':\n",
    "            best_plm_score_acter[model_name] = best_plm_score\n",
    "        elif dataset_name == 'ACL-RD':\n",
    "            best_plm_score_acl_rd[model_name] = best_plm_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "acter_short_ds = load_from_disk('dataset/ACTER/huggingface')\n",
    "for dataset_type in acter_short_ds:\n",
    "    ds = acter_short_ds[dataset_type]\n",
    "    all_num_words = 0\n",
    "    all_num_labels = 0\n",
    "    for row in ds:\n",
    "        words = row['text'].split(' ')\n",
    "        labels = row['label']\n",
    "        all_num_words += len(words)\n",
    "        all_num_labels += len(labels)\n",
    "    print(f'Short Dataset name: ACTER, Dataset type: {dataset_type}, Avg words: {all_num_words/len(ds)}, Avg labels: {all_num_labels/len(ds)}')\n",
    "\n",
    "print()   \n",
    "\n",
    "acl_rd_short_ds = load_from_disk('dataset/ACL-RD/huggingface')\n",
    "for dataset_path in acl_rd_short_ds:\n",
    "    ds = acl_rd_short_ds[dataset_path]\n",
    "    all_num_words = 0\n",
    "    all_num_labels = 0\n",
    "    for row in ds:\n",
    "        words = row['text'].split(' ')\n",
    "        labels = row['label']\n",
    "        all_num_words += len(words)\n",
    "        all_num_labels += len(labels)\n",
    "    print(f'Short Dataset name: ACL-RD, Dataset type: {dataset_path}, Avg words: {all_num_words/len(ds)}, Avg labels: {all_num_labels/len(ds)}')\n",
    "    \n",
    "print()\n",
    "\n",
    "bcgm_short_ds = load_from_disk('dataset/BCGM/huggingface')\n",
    "for dataset_path in bcgm_short_ds:\n",
    "    ds = bcgm_short_ds[dataset_path]\n",
    "    all_num_words = 0\n",
    "    all_num_labels = 0\n",
    "    for row in ds:\n",
    "        words = row['text'].split(' ')\n",
    "        labels = row['label']\n",
    "        all_num_words += len(words)\n",
    "        all_num_labels += len(labels)\n",
    "    print(f'Short Dataset name: BCGM, Dataset type: {dataset_path}, Avg words: {all_num_words/len(ds)}, Avg labels: {all_num_labels/len(ds)}')\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
