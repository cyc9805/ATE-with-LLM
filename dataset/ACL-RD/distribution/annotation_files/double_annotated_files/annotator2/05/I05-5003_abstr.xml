<?xml version="1.0" encoding="UTF-8" standalone="no"?><Paper acl-id="I05-5003">  <Title>Using Machine Translation Evaluation Techniques to Determine Sentence-level Semantic Equivalence</Title>  <Section>    <SectionTitle>Abstract</SectionTitle>    <S>The task of <term class="tech">machine translation (MT) evaluation</term> is closely related to the task of <term class="tech">sentence-level semantic equivalence classification</term>.</S>    <S>This paper investigates the utility of applying standard <term class="tech">MT evaluation methods</term> (<term class="measure(ment)">BLEU</term>, <term class="measure(ment)">NIST</term>, <term class="measure(ment)">WER</term> and <term class="measure(ment)">PER</term>) to building <term class="tech">classifiers</term> to predict <term class="other">semantic equivalence</term> and <term class="other">entailment</term>.</S>    <S>We also introduce a novel <term class="tech">classification method</term> based on <term class="measure(ment)">PER</term> which leverages <term class="other">part of speech information</term> of the words contributing to the <term class="other">word matches</term> and non-matches in the <term class="other">sentence</term>.</S>    <S>Our results show that <term class="tech">MT evaluation techniques</term> are able to produce useful features for <term class="tech">paraphrase classification</term> and to a lesser extent <term class="other">entailment</term>.</S>    <S>Our technique gives a substantial improvement in <term class="tech">paraphrase classification accuracy</term> over all of the other models used in the experiments.</S>  </Section></Paper>