<?xml version="1.0" encoding="UTF-8" standalone="no"?><Paper acl-id="H05-1117">  <Title>Automatically Evaluating Answers to Definition Questions</Title>  <Section>    <SectionTitle>Abstract</SectionTitle>    <S>Following recent developments in the <term class="tech">automatic evaluation</term> of <term class="tech">machine translation</term> and <term class="tech">document summarization</term>, we present a similar approach, implemented in a measure called <term class="measure(ment)">POURPRE</term>, for automatically evaluating answers to <term class="other">definition questions</term>.</S>    <S>Until now, the only way to assess the correctness of answers to such questions involves manual determination of whether an <term class="other">information nugget</term> appears in a system's response.</S>    <S>The lack of <term class="tech">automatic methods for scoring system output</term> is an impediment to progress in the field, which we address with this work.</S>    <S>Experiments with the <term class="other">TREC 2003 and TREC 2004 QA tracks</term> indicate that <term class="other">rankings</term> produced by our metric correlate highly with official <term class="other">rankings</term>, and that <term class="measure(ment)">POURPRE</term> outperforms direct application of existing metrics.</S>  </Section></Paper>