{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  \\\n",
      "0     Phenotypic analysis demonstrates that trio and...   \n",
      "1     In the cervical enlargement they were located ...   \n",
      "2     We measured electromyograms (EMG) of the alae ...   \n",
      "3     A case control study (1:2) of 182 pairs of Hep...   \n",
      "4     In the absence of shock, sepsis, or other iden...   \n",
      "...                                                 ...   \n",
      "4995  CONCLUSIONS: This initial experience indicates...   \n",
      "4996  A Brassica cDNA clone encoding a bifunctional ...   \n",
      "4997  The aims of this study were to examine whether...   \n",
      "4998  The goal of our work was to determine hearing ...   \n",
      "4999  Causal modeling combines theory and research, ...   \n",
      "\n",
      "                   position  \\\n",
      "0      [[34, 37], [41, 43]]   \n",
      "1                    [[-1]]   \n",
      "2                    [[-1]]   \n",
      "3                    [[-1]]   \n",
      "4                [[85, 94]]   \n",
      "...                     ...   \n",
      "4995                 [[-1]]   \n",
      "4996  [[39, 67], [69, 102]]   \n",
      "4997                 [[-1]]   \n",
      "4998                 [[-1]]   \n",
      "4999                 [[-1]]   \n",
      "\n",
      "                                                  label alt_position  \\\n",
      "0                                           [trio, Abl]       [[-1]]   \n",
      "1                                             [No term]       [[-1]]   \n",
      "2                                             [No term]       [[-1]]   \n",
      "3                                             [No term]       [[-1]]   \n",
      "4                                          [hemoglobin]       [[-1]]   \n",
      "...                                                 ...          ...   \n",
      "4995                                          [No term]       [[-1]]   \n",
      "4996  [hydroxymethylpyrimidine kinase, thiamin-phosp...  [[39, 102]]   \n",
      "4997                                          [No term]       [[-1]]   \n",
      "4998                                          [No term]       [[-1]]   \n",
      "4999                                          [No term]       [[-1]]   \n",
      "\n",
      "                                              alt_label  \n",
      "0                                             [No term]  \n",
      "1                                             [No term]  \n",
      "2                                             [No term]  \n",
      "3                                             [No term]  \n",
      "4                                             [No term]  \n",
      "...                                                 ...  \n",
      "4995                                          [No term]  \n",
      "4996  [hydroxymethylpyrimidine kinase/thiamin-phosph...  \n",
      "4997                                          [No term]  \n",
      "4998                                          [No term]  \n",
      "4999                                          [No term]  \n",
      "\n",
      "[5000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "def parse_sentences(file_path):\n",
    "    sentences = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Split the line into sentence ID and sentence\n",
    "            parts = line.strip().split(' ', 1)\n",
    "            if len(parts) == 2:\n",
    "                sentence_id, sentence = parts\n",
    "                sentences.append((sentence_id, sentence))\n",
    "    return sentences\n",
    "\n",
    "def parse_gene_eval(file_path):\n",
    "    terms = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Split the line into sentence ID, term position, and term\n",
    "            parts = line.strip().split('|')\n",
    "            if len(parts) == 3:\n",
    "                sentence_id, position, term = parts\n",
    "                start_pos, end_pos = map(int, position.split())\n",
    "                terms.append((sentence_id, [start_pos, end_pos], term))\n",
    "    return terms\n",
    "\n",
    "def parse_altgene_eval(file_path):\n",
    "    alt_terms = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Split the line into sentence ID, term position, and term\n",
    "            parts = line.strip().split('|')\n",
    "            if len(parts) == 3:\n",
    "                sentence_id, position, term = parts\n",
    "                start_pos, end_pos = map(int, position.split())\n",
    "                alt_terms.append((sentence_id, [start_pos, end_pos], term))\n",
    "    return alt_terms\n",
    "\n",
    "# Example usage\n",
    "sentences_path = 'original-data/test/test/test.in'  # Replace with the path to your file\n",
    "gene_eval_path = 'original-data/test/test/GENE.eval'  # Replace with the path to your file\n",
    "altgene_eval_path = 'original-data/test/test/ALTGENE.eval'  # Replace with the path to your file\n",
    "\n",
    "# Parse the sentences, terms, and alternative terms\n",
    "parsed_sentences = parse_sentences(sentences_path)\n",
    "parsed_terms = parse_gene_eval(gene_eval_path)\n",
    "parsed_alt_terms = parse_altgene_eval(altgene_eval_path)\n",
    "\n",
    "# Create DataFrames\n",
    "df = pd.DataFrame(parsed_sentences, columns=['Sentence ID', 'Sentence'])\n",
    "terms_df = pd.DataFrame(parsed_terms, columns=['Sentence ID', 'Position', 'Term'])\n",
    "alt_terms_df = pd.DataFrame(parsed_alt_terms, columns=['Sentence ID', 'Alt Position', 'Alt Term'])\n",
    "\n",
    "# Merge the DataFrames\n",
    "merged_df = df.merge(terms_df, on='Sentence ID', how='left')\n",
    "\n",
    "# Group by Sentence ID and aggregate terms and positions into lists\n",
    "merged_df = merged_df.groupby('Sentence ID').agg({\n",
    "    'Sentence': 'first',\n",
    "    'Position': lambda x: list(x.dropna()) if len(x.dropna()) > 0 else [[-1]],\n",
    "    'Term': lambda x: list(x.dropna()) if len(x.dropna()) > 0 else ['No term'],\n",
    "}).reset_index()\n",
    "\n",
    "merged_df = merged_df.merge(alt_terms_df, on='Sentence ID', how='left')\n",
    "\n",
    "# Group by Sentence ID and aggregate terms and positions into lists\n",
    "grouped_test_df = merged_df.groupby('Sentence ID').agg({\n",
    "    'Sentence': 'first',\n",
    "    'Position': 'first',\n",
    "    'Term': 'first',\n",
    "    'Alt Position': lambda x: list(x.dropna()) if len(x.dropna()) > 0 else [[-1]],\n",
    "    'Alt Term': lambda x: list(x.dropna()) if len(x.dropna()) > 0 else ['No term']\n",
    "}).reset_index()\n",
    "\n",
    "grouped_test_df.rename(columns={'Sentence':'text', 'Position': 'position', 'Term': 'label', 'Alt Position': 'alt_position', 'Alt Term': 'alt_label'}, inplace=True)\n",
    "grouped_test_df.drop('Sentence ID', axis=1, inplace=True)\n",
    "# Display the grouped DataFrame\n",
    "print(grouped_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "sentences_path = 'original-data/train/train/train.in'  # Replace with the path to your file\n",
    "gene_eval_path = 'original-data/train/train/GENE.eval'  # Replace with the path to your file\n",
    "altgene_eval_path = 'original-data/train/train/ALTGENE.eval'  # Replace with the path to your file\n",
    "\n",
    "# Parse the sentences, terms, and alternative terms\n",
    "parsed_sentences = parse_sentences(sentences_path)\n",
    "parsed_terms = parse_gene_eval(gene_eval_path)\n",
    "parsed_alt_terms = parse_altgene_eval(altgene_eval_path)\n",
    "\n",
    "# Create DataFrames\n",
    "df = pd.DataFrame(parsed_sentences, columns=['Sentence ID', 'Sentence'])\n",
    "terms_df = pd.DataFrame(parsed_terms, columns=['Sentence ID', 'Position', 'Term'])\n",
    "alt_terms_df = pd.DataFrame(parsed_alt_terms, columns=['Sentence ID', 'Alt Position', 'Alt Term'])\n",
    "\n",
    "# Merge the DataFrames\n",
    "merged_df = df.merge(terms_df, on='Sentence ID', how='left')\n",
    "\n",
    "# Group by Sentence ID and aggregate terms and positions into lists\n",
    "merged_df = merged_df.groupby('Sentence ID').agg({\n",
    "    'Sentence': 'first',\n",
    "    'Position': lambda x: list(x.dropna()) if len(x.dropna()) > 0 else [[-1]],\n",
    "    'Term': lambda x: list(x.dropna()) if len(x.dropna()) > 0 else ['No term'],\n",
    "}).reset_index()\n",
    "\n",
    "merged_df = merged_df.merge(alt_terms_df, on='Sentence ID', how='left')\n",
    "\n",
    "# Group by Sentence ID and aggregate terms and positions into lists\n",
    "grouped_train_df = merged_df.groupby('Sentence ID').agg({\n",
    "    'Sentence': 'first',\n",
    "    'Position': 'first',\n",
    "    'Term': 'first',\n",
    "    'Alt Position': lambda x: list(x.dropna()) if len(x.dropna()) > 0 else [[-1]],\n",
    "    'Alt Term': lambda x: list(x.dropna()) if len(x.dropna()) > 0 else ['No term']\n",
    "}).reset_index()\n",
    "\n",
    "grouped_train_df.rename(columns={'Sentence':'text', 'Position': 'position', 'Term': 'label', 'Alt Position': 'alt_position', 'Alt Term': 'alt_label'}, inplace=True)\n",
    "grouped_train_df.drop('Sentence ID', axis=1, inplace=True)\n",
    "\n",
    "# Display the grouped DataFrame\n",
    "print(grouped_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add domain column\n",
    "grouped_train_df['domain'] = 'Gene'\n",
    "grouped_test_df['domain'] = 'Gene'\n",
    "\n",
    "grouped_train_df['category'] = 'unknown'\n",
    "grouped_test_df['category'] = 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length: 974\n"
     ]
    }
   ],
   "source": [
    "# Find the row with the longest sentence\n",
    "max_length = grouped_train_df['text'].apply(len).max()\n",
    "print('Max sentence length:', max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text            When the CO2 content reached 9 Vol% the animal...\n",
       "position                                                   [[-1]]\n",
       "label                                                   [No term]\n",
       "alt_position                                               [[-1]]\n",
       "alt_label                                               [No term]\n",
       "domain                                                       Gene\n",
       "category                                                  unknown\n",
       "Name: 10, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_train_df.iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Convert the grouped DataFrames to datasets\n",
    "train_dataset = Dataset.from_pandas(grouped_train_df)\n",
    "test_dataset = Dataset.from_pandas(grouped_test_df)\n",
    "\n",
    "# Split the train dataset into train and validation sets\n",
    "train_dataset = train_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "validation_dataset = train_dataset['test']\n",
    "train_dataset = train_dataset['train']\n",
    "\n",
    "# Save the datasets\n",
    "save_dir = 'huggingface/short'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "dataset_dict = DatasetDict({\"train\": train_dataset, \"validation\": validation_dataset, \"test\": test_dataset})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_label = []\n",
    "for l, al in zip(test_dataset['label'], test_dataset['alt_label']):\n",
    "    new_label.append(l+al)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 21]]\n",
      "['COR biosynthetic gene']\n",
      "[[3, 5], [138, 140]]\n",
      "The COR biosynthetic gene cluster in P. syringae pv. glycinea PG4180 is encoded by a 32-kb region which contains both the structural and regulatory genes needed for COR synthesis.\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset['alt_position'][6])\n",
    "print(test_dataset['alt_label'][6])\n",
    "print(test_dataset['position'][6])\n",
    "print(test_dataset['text'][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['No term']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_new_ref_tags = []\n",
    "for i, [pos, alt_pos] in enumerate(zip(test_dataset['position'], test_dataset['alt_position'])):\n",
    "    new_ref_tags = []\n",
    "    if pos[0] == -1 and alt_pos[0] == -1:\n",
    "        new_ref_tags.append(['No term'])\n",
    "    else:\n",
    "        for j, p in enumerate(pos):\n",
    "            label = test_dataset['label'][i][j]\n",
    "            \n",
    "            alt_tags = [label]\n",
    "                \n",
    "            for k, ap in enumerate(alt_pos):\n",
    "                if ap[0] == -1:\n",
    "                    continue\n",
    "                elif p[0] == -1:\n",
    "                    alt_label = test_dataset['alt_label'][i][k]\n",
    "                    alt_tags.append(alt_label)\n",
    "                elif max(p[0], ap[0]) < min(p[1], ap[1]):\n",
    "                    alt_label = test_dataset['alt_label'][i][k]\n",
    "                    alt_tags.append(alt_label)\n",
    "            new_ref_tags.append(alt_tags)\n",
    "    total_new_ref_tags.append(new_ref_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THis is idx: 10\n",
      "[[-1]]\n",
      "This is alt label: ['No term']\n",
      "[[19, 22], [30, 34], [71, 74]]\n",
      "This is label: ['MAPK', 'Raf-1', 'Jak1']\n",
      "Interestingly, basal MAPK, but not Raf-1, activity was constitutively enhanced in Jak1-deficient HeLa cells.\n",
      "[['MAPK'], ['Raf-1'], ['Jak1']]\n",
      "\n",
      "THis is idx: 11\n",
      "[[-1]]\n",
      "This is alt label: ['No term']\n",
      "[[-1]]\n",
      "This is label: ['No term']\n",
      "On the other hand hypokalemia, induced by diuretics, may also be accompanied by a significant depletion of total body K, bringing about more general consequences.\n",
      "[['No term']]\n",
      "\n",
      "THis is idx: 12\n",
      "[[47, 51]]\n",
      "This is alt label: ['gp330']\n",
      "[[42, 51]]\n",
      "This is label: ['human gp330']\n",
      "We present here the complete primary structure of human gp330, the human variant of the principal kidney autoantigen causing Heymann membranous glomerulonephritis in rats.\n",
      "[['human gp330', 'gp330']]\n",
      "\n",
      "THis is idx: 13\n",
      "[[-1]]\n",
      "This is alt label: ['No term']\n",
      "[[-1]]\n",
      "This is label: ['No term']\n",
      "A series of deletion mutants was expressed transiently in two human hepatocytes, HepG2 and PLC.\n",
      "[['No term']]\n",
      "\n",
      "THis is idx: 14\n",
      "[[-1]]\n",
      "This is alt label: ['No term']\n",
      "[[-1]]\n",
      "This is label: ['No term']\n",
      "Biol.\n",
      "[['No term']]\n",
      "\n",
      "THis is idx: 15\n",
      "[[-1]]\n",
      "This is alt label: ['No term']\n",
      "[[-1]]\n",
      "This is label: ['No term']\n",
      "Copyright 2000 Academic Press.\n",
      "[['No term']]\n",
      "\n",
      "THis is idx: 16\n",
      "[[70, 87]]\n",
      "This is alt label: ['procollagen (PC III)']\n",
      "[[70, 80], [82, 86]]\n",
      "This is label: ['procollagen', 'PC III']\n",
      "No patient demonstrated a decrease in bone marrow fibrosis as determined by serial procollagen (PC III) serum level analysis.\n",
      "[['procollagen', 'procollagen (PC III)'], ['PC III', 'procollagen (PC III)']]\n",
      "\n",
      "THis is idx: 17\n",
      "[[51, 60]]\n",
      "This is alt label: ['pepsinogen']\n",
      "[[46, 60]]\n",
      "This is label: ['serum pepsinogen']\n",
      "In four calves given Haemonchus contortus larvae, the serum pepsinogen concentration rose quickly to reach a mean of 3.5 iu tyrosine on day 14 after infection.\n",
      "[['serum pepsinogen', 'pepsinogen']]\n",
      "\n",
      "THis is idx: 18\n",
      "[[3, 8], [3, 38]]\n",
      "This is alt label: ['P-ITIM', 'P-ITIM-compelled multi-phosphoprotein']\n",
      "[[3, 45], [65, 69], [98, 101], [105, 107]]\n",
      "This is label: ['P-ITIM-compelled multi-phosphoprotein complex', 'SHP-2', 'SHIP', 'Shc']\n",
      "The P-ITIM-compelled multi-phosphoprotein complex binds to and activates SHP-2, which in turn dephosphorylates SHIP and Shc and probably other substrates.\n",
      "[['P-ITIM-compelled multi-phosphoprotein complex', 'P-ITIM', 'P-ITIM-compelled multi-phosphoprotein'], ['SHP-2'], ['SHIP'], ['Shc']]\n",
      "\n",
      "THis is idx: 19\n",
      "[[171, 172]]\n",
      "This is alt label: ['Gs']\n",
      "[[27, 50], [73, 76], [114, 116], [171, 179]]\n",
      "This is label: ['M1Ach-muscarinic receptor', 'GnRH', 'PRL', 'Gs protein']\n",
      "Paradoxically, loop 3i from the M1Ach-muscarinic receptor also maximally inhibited GnRH agonist-stimulated cAMP accumulation and PRL release by 40% (both effects mediated through activation of the Gs protein).\n",
      "[['M1Ach-muscarinic receptor'], ['GnRH'], ['PRL'], ['Gs protein', 'Gs']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx in range(10,20):\n",
    "    print('THis is idx:', idx)\n",
    "    print(test_dataset['alt_position'][idx])\n",
    "    print('This is alt label:', test_dataset['alt_label'][idx])\n",
    "    print(test_dataset['position'][idx])\n",
    "    print('This is label:', test_dataset['label'][idx])\n",
    "    print(test_dataset['text'][idx])\n",
    "    print(total_new_ref_tags[idx])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'position', 'label', 'alt_position', 'alt_label', 'domain'],\n",
       "    num_rows: 5000\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'position', 'label', 'alt_position', 'alt_label', 'domain', 'relaxed_labels'],\n",
       "    num_rows: 5000\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add total_new_ref_tags to the test_dataset\n",
    "test_dataset = test_dataset.add_column('relaxed_labels', total_new_ref_tags)\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 12000/12000 [00:00<00:00, 53348.22 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3000/3000 [00:00<00:00, 67503.44 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 5000/5000 [00:00<00:00, 85160.77 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_dict = DatasetDict({\"train\": train_dataset, \"validation\": validation_dataset, \"test\": test_dataset})\n",
    "dataset_dict.save_to_disk(save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ATE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
