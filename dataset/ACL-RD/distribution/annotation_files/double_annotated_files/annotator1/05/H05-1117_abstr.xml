<?xml version="1.0" encoding="UTF-8" standalone="no"?><Paper acl-id="H05-1117">  <Title>Automatically Evaluating Answers to Definition Questions</Title>  <Section>    <SectionTitle>Abstract</SectionTitle> <!-- BQ: is automatic evaluation category 6 or 1?! previously it was 1 as it refers to a method but now is 6, right? -->    <S>Following recent developments in the <term class="tech">automatic evaluation</term> of <term class="tech">machine translation</term> and <term class="tech">document summarization</term>, we present a similar approach, implemented in a measure called <term class="measure(ment)">POURPRE</term>, for <term class="measure(ment)">automatically evaluating answers to definition questions</term>.</S>    <S>Until now, the only way to assess the correctness of answers to such questions involves manual determination of whether an information nugget appears in a system's response.</S>    <S>The lack of automatic methods for <term class="measure(ment)">scoring system output</term> is an impediment to progress in the field, which we address with this work.</S>    <S>Experiments with the <term class="other">TREC 2003 and TREC 2004 QA tracks</term> indicate that <term class="other">rankings</term> produced by our metric correlate highly with <term class="measure(ment)">official rankings</term>, and that <term class="measure(ment)">POURPRE</term> outperforms direct application of existing metrics.</S>  </Section></Paper>