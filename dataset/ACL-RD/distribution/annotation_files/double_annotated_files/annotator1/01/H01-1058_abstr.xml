<?xml version="1.0" encoding="UTF-8" standalone="no"?><Paper acl-id="H01-1058">	<Title>On Combining Language Models : Oracle Approach</Title>		<Section>			<SectionTitle>ABSTRACT</SectionTitle>			<S>In this paper, we address the problem of combining several <term class="model">language models (LMs)</term>.</S>			<S>We find that simple <term class="tech">interpolation methods</term>, like <term class="tech">log-linear and linear interpolation</term>, improve the <term class="measure(ment)">performance</term> but fall short of the <term class="measure(ment)">performance</term> of an <term class="other">oracle</term>.</S>			<S>The <term class="other">oracle</term> knows the <term class="other">reference word string</term> and selects the <term class="other">word string</term> with the best <term class="measure(ment)">performance</term> (typically, <term class="measure(ment)">word or semantic error rate</term>) from a list of <term class="other">word strings</term>, where each <term class="other">word string</term> has been obtained by using a different <term class="model">LM</term>.</S>			<S>Actually, the <term class="other">oracle</term> acts like a <term class="tech">dynamic combiner</term> with <term class="other">hard decisions</term> using the <term class="other">reference</term>.</S>			<S>We provide experimental results that clearly show the need for a <term class="tech">dynamic language model combination</term> to improve the <term class="measure(ment)">performance</term> further.</S>			<S>We suggest a method that mimics the behavior of the <term class="other">oracle</term> using a <term class="tech">neural network</term> or a <term class="tech">decision tree</term>.</S>			<S>The method amounts to tagging <term class="model">LMs</term> with <term class="measure(ment)">confidence measures</term> and picking the best <term class="other">hypothesis</term> corresponding to the <term class="model">LM</term> with the best <term class="measure(ment)">confidence</term>.</S>		</Section></Paper>