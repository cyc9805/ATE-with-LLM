<?xml version="1.0" encoding="UTF-8" standalone="no"?><Paper acl-id="P05-1010">  <Title>Probabilistic CFG with latent annotations</Title>  <Section>    <SectionTitle>Abstract</SectionTitle>    <S>This paper defines a <term class="other">generative probabilistic model</term> of <term class="other">parse trees</term>, which we call <term class="other">PCFG-LA</term>.</S>    <S>This <term class="other">model</term> is an extension of <term class="other">PCFG</term> in which <term class="other">non-terminal symbols</term> are augmented with <term class="other">latent variables</term>.</S>    <S>         <term class="model">Fine-grained CFG rules</term> are automatically induced from a <term class="lr">parsed corpus</term> by training a <term class="model">PCFG-LA model</term> using an <term class="tech">EM-algorithm</term>.</S>    <S>Because <term class="tech">exact parsing</term> with a <term class="other">PCFG-LA</term> is NP-hard, several approximations are described and empirically compared.</S>    <S>In experiments using the <term class="lr-prod">Penn WSJ corpus</term>, our <term class="model">automatically trained model</term> gave a performance of 86.6% (Fa5 , sentences a6 40 words), which is comparable to that of an <term class="tech">unlexicalized PCFG parser</term> created using extensive <term class="tech">manual feature selection</term>.</S>  </Section></Paper><!--ak did NOT correct mathematical symbols, 7.12.15-->